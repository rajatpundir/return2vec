{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div class=\"bk-root\">\n",
       "        <a href=\"https://bokeh.pydata.org\" target=\"_blank\" class=\"bk-logo bk-logo-small bk-logo-notebook\"></a>\n",
       "        <span id=\"1001\">Loading BokehJS ...</span>\n",
       "    </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "(function(root) {\n",
       "  function now() {\n",
       "    return new Date();\n",
       "  }\n",
       "\n",
       "  var force = true;\n",
       "\n",
       "  if (typeof root._bokeh_onload_callbacks === \"undefined\" || force === true) {\n",
       "    root._bokeh_onload_callbacks = [];\n",
       "    root._bokeh_is_loading = undefined;\n",
       "  }\n",
       "\n",
       "  var JS_MIME_TYPE = 'application/javascript';\n",
       "  var HTML_MIME_TYPE = 'text/html';\n",
       "  var EXEC_MIME_TYPE = 'application/vnd.bokehjs_exec.v0+json';\n",
       "  var CLASS_NAME = 'output_bokeh rendered_html';\n",
       "\n",
       "  /**\n",
       "   * Render data to the DOM node\n",
       "   */\n",
       "  function render(props, node) {\n",
       "    var script = document.createElement(\"script\");\n",
       "    node.appendChild(script);\n",
       "  }\n",
       "\n",
       "  /**\n",
       "   * Handle when an output is cleared or removed\n",
       "   */\n",
       "  function handleClearOutput(event, handle) {\n",
       "    var cell = handle.cell;\n",
       "\n",
       "    var id = cell.output_area._bokeh_element_id;\n",
       "    var server_id = cell.output_area._bokeh_server_id;\n",
       "    // Clean up Bokeh references\n",
       "    if (id != null && id in Bokeh.index) {\n",
       "      Bokeh.index[id].model.document.clear();\n",
       "      delete Bokeh.index[id];\n",
       "    }\n",
       "\n",
       "    if (server_id !== undefined) {\n",
       "      // Clean up Bokeh references\n",
       "      var cmd = \"from bokeh.io.state import curstate; print(curstate().uuid_to_server['\" + server_id + \"'].get_sessions()[0].document.roots[0]._id)\";\n",
       "      cell.notebook.kernel.execute(cmd, {\n",
       "        iopub: {\n",
       "          output: function(msg) {\n",
       "            var id = msg.content.text.trim();\n",
       "            if (id in Bokeh.index) {\n",
       "              Bokeh.index[id].model.document.clear();\n",
       "              delete Bokeh.index[id];\n",
       "            }\n",
       "          }\n",
       "        }\n",
       "      });\n",
       "      // Destroy server and session\n",
       "      var cmd = \"import bokeh.io.notebook as ion; ion.destroy_server('\" + server_id + \"')\";\n",
       "      cell.notebook.kernel.execute(cmd);\n",
       "    }\n",
       "  }\n",
       "\n",
       "  /**\n",
       "   * Handle when a new output is added\n",
       "   */\n",
       "  function handleAddOutput(event, handle) {\n",
       "    var output_area = handle.output_area;\n",
       "    var output = handle.output;\n",
       "\n",
       "    // limit handleAddOutput to display_data with EXEC_MIME_TYPE content only\n",
       "    if ((output.output_type != \"display_data\") || (!output.data.hasOwnProperty(EXEC_MIME_TYPE))) {\n",
       "      return\n",
       "    }\n",
       "\n",
       "    var toinsert = output_area.element.find(\".\" + CLASS_NAME.split(' ')[0]);\n",
       "\n",
       "    if (output.metadata[EXEC_MIME_TYPE][\"id\"] !== undefined) {\n",
       "      toinsert[toinsert.length - 1].firstChild.textContent = output.data[JS_MIME_TYPE];\n",
       "      // store reference to embed id on output_area\n",
       "      output_area._bokeh_element_id = output.metadata[EXEC_MIME_TYPE][\"id\"];\n",
       "    }\n",
       "    if (output.metadata[EXEC_MIME_TYPE][\"server_id\"] !== undefined) {\n",
       "      var bk_div = document.createElement(\"div\");\n",
       "      bk_div.innerHTML = output.data[HTML_MIME_TYPE];\n",
       "      var script_attrs = bk_div.children[0].attributes;\n",
       "      for (var i = 0; i < script_attrs.length; i++) {\n",
       "        toinsert[toinsert.length - 1].firstChild.setAttribute(script_attrs[i].name, script_attrs[i].value);\n",
       "      }\n",
       "      // store reference to server id on output_area\n",
       "      output_area._bokeh_server_id = output.metadata[EXEC_MIME_TYPE][\"server_id\"];\n",
       "    }\n",
       "  }\n",
       "\n",
       "  function register_renderer(events, OutputArea) {\n",
       "\n",
       "    function append_mime(data, metadata, element) {\n",
       "      // create a DOM node to render to\n",
       "      var toinsert = this.create_output_subarea(\n",
       "        metadata,\n",
       "        CLASS_NAME,\n",
       "        EXEC_MIME_TYPE\n",
       "      );\n",
       "      this.keyboard_manager.register_events(toinsert);\n",
       "      // Render to node\n",
       "      var props = {data: data, metadata: metadata[EXEC_MIME_TYPE]};\n",
       "      render(props, toinsert[toinsert.length - 1]);\n",
       "      element.append(toinsert);\n",
       "      return toinsert\n",
       "    }\n",
       "\n",
       "    /* Handle when an output is cleared or removed */\n",
       "    events.on('clear_output.CodeCell', handleClearOutput);\n",
       "    events.on('delete.Cell', handleClearOutput);\n",
       "\n",
       "    /* Handle when a new output is added */\n",
       "    events.on('output_added.OutputArea', handleAddOutput);\n",
       "\n",
       "    /**\n",
       "     * Register the mime type and append_mime function with output_area\n",
       "     */\n",
       "    OutputArea.prototype.register_mime_type(EXEC_MIME_TYPE, append_mime, {\n",
       "      /* Is output safe? */\n",
       "      safe: true,\n",
       "      /* Index of renderer in `output_area.display_order` */\n",
       "      index: 0\n",
       "    });\n",
       "  }\n",
       "\n",
       "  // register the mime type if in Jupyter Notebook environment and previously unregistered\n",
       "  if (root.Jupyter !== undefined) {\n",
       "    var events = require('base/js/events');\n",
       "    var OutputArea = require('notebook/js/outputarea').OutputArea;\n",
       "\n",
       "    if (OutputArea.prototype.mime_types().indexOf(EXEC_MIME_TYPE) == -1) {\n",
       "      register_renderer(events, OutputArea);\n",
       "    }\n",
       "  }\n",
       "\n",
       "  \n",
       "  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n",
       "    root._bokeh_timeout = Date.now() + 5000;\n",
       "    root._bokeh_failed_load = false;\n",
       "  }\n",
       "\n",
       "  var NB_LOAD_WARNING = {'data': {'text/html':\n",
       "     \"<div style='background-color: #fdd'>\\n\"+\n",
       "     \"<p>\\n\"+\n",
       "     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n",
       "     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n",
       "     \"</p>\\n\"+\n",
       "     \"<ul>\\n\"+\n",
       "     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n",
       "     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n",
       "     \"</ul>\\n\"+\n",
       "     \"<code>\\n\"+\n",
       "     \"from bokeh.resources import INLINE\\n\"+\n",
       "     \"output_notebook(resources=INLINE)\\n\"+\n",
       "     \"</code>\\n\"+\n",
       "     \"</div>\"}};\n",
       "\n",
       "  function display_loaded() {\n",
       "    var el = document.getElementById(\"1001\");\n",
       "    if (el != null) {\n",
       "      el.textContent = \"BokehJS is loading...\";\n",
       "    }\n",
       "    if (root.Bokeh !== undefined) {\n",
       "      if (el != null) {\n",
       "        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\n",
       "      }\n",
       "    } else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(display_loaded, 100)\n",
       "    }\n",
       "  }\n",
       "\n",
       "\n",
       "  function run_callbacks() {\n",
       "    try {\n",
       "      root._bokeh_onload_callbacks.forEach(function(callback) {\n",
       "        if (callback != null)\n",
       "          callback();\n",
       "      });\n",
       "    } finally {\n",
       "      delete root._bokeh_onload_callbacks\n",
       "    }\n",
       "    console.debug(\"Bokeh: all callbacks have finished\");\n",
       "  }\n",
       "\n",
       "  function load_libs(css_urls, js_urls, callback) {\n",
       "    if (css_urls == null) css_urls = [];\n",
       "    if (js_urls == null) js_urls = [];\n",
       "\n",
       "    root._bokeh_onload_callbacks.push(callback);\n",
       "    if (root._bokeh_is_loading > 0) {\n",
       "      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n",
       "      return null;\n",
       "    }\n",
       "    if (js_urls == null || js_urls.length === 0) {\n",
       "      run_callbacks();\n",
       "      return null;\n",
       "    }\n",
       "    console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n",
       "    root._bokeh_is_loading = css_urls.length + js_urls.length;\n",
       "\n",
       "    function on_load() {\n",
       "      root._bokeh_is_loading--;\n",
       "      if (root._bokeh_is_loading === 0) {\n",
       "        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n",
       "        run_callbacks()\n",
       "      }\n",
       "    }\n",
       "\n",
       "    function on_error() {\n",
       "      console.error(\"failed to load \" + url);\n",
       "    }\n",
       "\n",
       "    for (var i = 0; i < css_urls.length; i++) {\n",
       "      var url = css_urls[i];\n",
       "      const element = document.createElement(\"link\");\n",
       "      element.onload = on_load;\n",
       "      element.onerror = on_error;\n",
       "      element.rel = \"stylesheet\";\n",
       "      element.type = \"text/css\";\n",
       "      element.href = url;\n",
       "      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n",
       "      document.body.appendChild(element);\n",
       "    }\n",
       "\n",
       "    for (var i = 0; i < js_urls.length; i++) {\n",
       "      var url = js_urls[i];\n",
       "      var element = document.createElement('script');\n",
       "      element.onload = on_load;\n",
       "      element.onerror = on_error;\n",
       "      element.async = false;\n",
       "      element.src = url;\n",
       "      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n",
       "      document.head.appendChild(element);\n",
       "    }\n",
       "  };var element = document.getElementById(\"1001\");\n",
       "  if (element == null) {\n",
       "    console.error(\"Bokeh: ERROR: autoload.js configured with elementid '1001' but no matching script tag was found. \")\n",
       "    return false;\n",
       "  }\n",
       "\n",
       "  function inject_raw_css(css) {\n",
       "    const element = document.createElement(\"style\");\n",
       "    element.appendChild(document.createTextNode(css));\n",
       "    document.body.appendChild(element);\n",
       "  }\n",
       "\n",
       "  var js_urls = [\"https://cdn.pydata.org/bokeh/release/bokeh-1.2.0.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-widgets-1.2.0.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-tables-1.2.0.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-gl-1.2.0.min.js\"];\n",
       "  var css_urls = [\"https://cdn.pydata.org/bokeh/release/bokeh-1.2.0.min.css\", \"https://cdn.pydata.org/bokeh/release/bokeh-widgets-1.2.0.min.css\", \"https://cdn.pydata.org/bokeh/release/bokeh-tables-1.2.0.min.css\"];\n",
       "\n",
       "  var inline_js = [\n",
       "    function(Bokeh) {\n",
       "      Bokeh.set_log_level(\"info\");\n",
       "    },\n",
       "    \n",
       "    function(Bokeh) {\n",
       "      \n",
       "    },\n",
       "    function(Bokeh) {} // ensure no trailing comma for IE\n",
       "  ];\n",
       "\n",
       "  function run_inline_js() {\n",
       "    \n",
       "    if ((root.Bokeh !== undefined) || (force === true)) {\n",
       "      for (var i = 0; i < inline_js.length; i++) {\n",
       "        inline_js[i].call(root, root.Bokeh);\n",
       "      }if (force === true) {\n",
       "        display_loaded();\n",
       "      }} else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(run_inline_js, 100);\n",
       "    } else if (!root._bokeh_failed_load) {\n",
       "      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n",
       "      root._bokeh_failed_load = true;\n",
       "    } else if (force !== true) {\n",
       "      var cell = $(document.getElementById(\"1001\")).parents('.cell').data().cell;\n",
       "      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n",
       "    }\n",
       "\n",
       "  }\n",
       "\n",
       "  if (root._bokeh_is_loading === 0) {\n",
       "    console.debug(\"Bokeh: BokehJS loaded, going straight to plotting\");\n",
       "    run_inline_js();\n",
       "  } else {\n",
       "    load_libs(css_urls, js_urls, function() {\n",
       "      console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n",
       "      run_inline_js();\n",
       "    });\n",
       "  }\n",
       "}(window));"
      ],
      "application/vnd.bokehjs_load.v0+json": "\n(function(root) {\n  function now() {\n    return new Date();\n  }\n\n  var force = true;\n\n  if (typeof root._bokeh_onload_callbacks === \"undefined\" || force === true) {\n    root._bokeh_onload_callbacks = [];\n    root._bokeh_is_loading = undefined;\n  }\n\n  \n\n  \n  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n    root._bokeh_timeout = Date.now() + 5000;\n    root._bokeh_failed_load = false;\n  }\n\n  var NB_LOAD_WARNING = {'data': {'text/html':\n     \"<div style='background-color: #fdd'>\\n\"+\n     \"<p>\\n\"+\n     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n     \"</p>\\n\"+\n     \"<ul>\\n\"+\n     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n     \"</ul>\\n\"+\n     \"<code>\\n\"+\n     \"from bokeh.resources import INLINE\\n\"+\n     \"output_notebook(resources=INLINE)\\n\"+\n     \"</code>\\n\"+\n     \"</div>\"}};\n\n  function display_loaded() {\n    var el = document.getElementById(\"1001\");\n    if (el != null) {\n      el.textContent = \"BokehJS is loading...\";\n    }\n    if (root.Bokeh !== undefined) {\n      if (el != null) {\n        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\n      }\n    } else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(display_loaded, 100)\n    }\n  }\n\n\n  function run_callbacks() {\n    try {\n      root._bokeh_onload_callbacks.forEach(function(callback) {\n        if (callback != null)\n          callback();\n      });\n    } finally {\n      delete root._bokeh_onload_callbacks\n    }\n    console.debug(\"Bokeh: all callbacks have finished\");\n  }\n\n  function load_libs(css_urls, js_urls, callback) {\n    if (css_urls == null) css_urls = [];\n    if (js_urls == null) js_urls = [];\n\n    root._bokeh_onload_callbacks.push(callback);\n    if (root._bokeh_is_loading > 0) {\n      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n      return null;\n    }\n    if (js_urls == null || js_urls.length === 0) {\n      run_callbacks();\n      return null;\n    }\n    console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n    root._bokeh_is_loading = css_urls.length + js_urls.length;\n\n    function on_load() {\n      root._bokeh_is_loading--;\n      if (root._bokeh_is_loading === 0) {\n        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n        run_callbacks()\n      }\n    }\n\n    function on_error() {\n      console.error(\"failed to load \" + url);\n    }\n\n    for (var i = 0; i < css_urls.length; i++) {\n      var url = css_urls[i];\n      const element = document.createElement(\"link\");\n      element.onload = on_load;\n      element.onerror = on_error;\n      element.rel = \"stylesheet\";\n      element.type = \"text/css\";\n      element.href = url;\n      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n      document.body.appendChild(element);\n    }\n\n    for (var i = 0; i < js_urls.length; i++) {\n      var url = js_urls[i];\n      var element = document.createElement('script');\n      element.onload = on_load;\n      element.onerror = on_error;\n      element.async = false;\n      element.src = url;\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.head.appendChild(element);\n    }\n  };var element = document.getElementById(\"1001\");\n  if (element == null) {\n    console.error(\"Bokeh: ERROR: autoload.js configured with elementid '1001' but no matching script tag was found. \")\n    return false;\n  }\n\n  function inject_raw_css(css) {\n    const element = document.createElement(\"style\");\n    element.appendChild(document.createTextNode(css));\n    document.body.appendChild(element);\n  }\n\n  var js_urls = [\"https://cdn.pydata.org/bokeh/release/bokeh-1.2.0.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-widgets-1.2.0.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-tables-1.2.0.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-gl-1.2.0.min.js\"];\n  var css_urls = [\"https://cdn.pydata.org/bokeh/release/bokeh-1.2.0.min.css\", \"https://cdn.pydata.org/bokeh/release/bokeh-widgets-1.2.0.min.css\", \"https://cdn.pydata.org/bokeh/release/bokeh-tables-1.2.0.min.css\"];\n\n  var inline_js = [\n    function(Bokeh) {\n      Bokeh.set_log_level(\"info\");\n    },\n    \n    function(Bokeh) {\n      \n    },\n    function(Bokeh) {} // ensure no trailing comma for IE\n  ];\n\n  function run_inline_js() {\n    \n    if ((root.Bokeh !== undefined) || (force === true)) {\n      for (var i = 0; i < inline_js.length; i++) {\n        inline_js[i].call(root, root.Bokeh);\n      }if (force === true) {\n        display_loaded();\n      }} else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(run_inline_js, 100);\n    } else if (!root._bokeh_failed_load) {\n      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n      root._bokeh_failed_load = true;\n    } else if (force !== true) {\n      var cell = $(document.getElementById(\"1001\")).parents('.cell').data().cell;\n      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n    }\n\n  }\n\n  if (root._bokeh_is_loading === 0) {\n    console.debug(\"Bokeh: BokehJS loaded, going straight to plotting\");\n    run_inline_js();\n  } else {\n    load_libs(css_urls, js_urls, function() {\n      console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n      run_inline_js();\n    });\n  }\n}(window));"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import collections\n",
    "import random\n",
    "import math\n",
    "import click\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from six.moves import xrange\n",
    "import mlflow\n",
    "from scipy.spatial import distance\n",
    "from scipy.stats import kurtosis, skew\n",
    "from bokeh.plotting import figure, show, output_notebook\n",
    "output_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mlflow.create_experiment('Model')\n",
    "model_experiment = 1\n",
    "# mlflow.create_experiment('Returns')\n",
    "returns_experiment = 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Word2Vec:\n",
    "    def __init__(self,filename\n",
    "                 , vocabulary_size=50000\n",
    "                 , batch_size=128\n",
    "                 , skip_window=1\n",
    "                 , num_skips=2\n",
    "                 , embedding_size = 128\n",
    "                 , num_sampled = 64\n",
    "                 , num_steps = 100001\n",
    "                ):\n",
    "        self.filename = filename\n",
    "        self.vocabulary_size = vocabulary_size\n",
    "        self.data_index = 0\n",
    "        self.batch_size = batch_size\n",
    "        self.skip_window = skip_window  # Number of words to consider left and right.\n",
    "        self.num_skips = num_skips # Number of times to reuse a window to generate a label\n",
    "        # self.embedding_size is the length of the word vector, empirically it should be in range [50, 2000].\n",
    "        self.embedding_size = embedding_size\n",
    "        self.num_sampled = num_sampled  # Number of negative examples to sample.\n",
    "        self.num_steps = num_steps # Number of steps the model will be trained for.\n",
    "        # The below is only for displaynig model accuracy, remove this in production.\n",
    "        # We pick a random validation set to sample nearest neighbors. \n",
    "        # Here we limit the validation samples to the words that have a low numeric ID, which by construction are also the most frequent. \n",
    "        # These 3 variables are used only for displaying model accuracy, they don't affect calculation.\n",
    "        self.valid_size = 16  # Random set of words to evaluate similarity on.\n",
    "        self.valid_window = 100  # Only pick samples in the head of the distribution.\n",
    "        self.valid_examples = np.random.choice(self.valid_window, self.valid_size, replace=False)\n",
    "        self.min_loss = 99999999999\n",
    "        self.best_embeddings = None\n",
    "\n",
    "    def run(self):\n",
    "        self.load_vocabulary()\n",
    "        self.build_dataset()\n",
    "        self.build_model()\n",
    "        self.train_model()\n",
    "\n",
    "    def load_vocabulary(self):\n",
    "        self.vocabulary = []\n",
    "        f = open(self.filename, 'r')\n",
    "        for l in f.readlines():\n",
    "            for r in l.split(\" \"):\n",
    "                if r.strip() == \"\":\n",
    "                    continue\n",
    "                self.vocabulary.append(r)\n",
    "        f.close()\n",
    "        print('Success: Loaded Vocabulary.')\n",
    "        print('Vocabulary size :', len(self.vocabulary))\n",
    "\n",
    "    def build_dataset(self):\n",
    "        # Compose count (a list of lists which maps top n words to their frequencies in a descending fashion)\n",
    "        k = []\n",
    "        k.extend(collections.Counter(self.vocabulary).most_common(len(set(self.vocabulary))))\n",
    "        count = [['UNK', -1]]\n",
    "        for x in k:\n",
    "            if x[1] >= 2:\n",
    "                count.append(x)\n",
    "        self.vocabulary_size = math.floor(len(count))\n",
    "        dictionary = {}\n",
    "        # Compose dictionary (a one-to-one mapping of words to some integer)\n",
    "        for word, _ in count:\n",
    "            dictionary[word] = len(dictionary)\n",
    "        data = []\n",
    "        unk_count = 0\n",
    "        # Compose data (a list of integers (corresponding to the each word in vocabulary))\n",
    "        for word in self.vocabulary:\n",
    "            index = dictionary.get(word, 0)\n",
    "            # If index equals zero, then its a rare word(UNK).\n",
    "            if index == 0:\n",
    "                unk_count += 1\n",
    "            data.append(index)\n",
    "        # Update the frequency of rare words(UNK) from -1.\n",
    "        count[0][1] = unk_count\n",
    "        # Compose reversed_dictionary ( a one-to-one mapping of integer codes to their string representation(words))\n",
    "        reverse_dictionary = dict(zip(dictionary.values(), dictionary.keys()))\n",
    "        self.data = data\n",
    "        self.count = count\n",
    "        self.unused_dictionary = dictionary\n",
    "        self.reverse_dictionary = reverse_dictionary\n",
    "        print('Success: Built Dataset.')\n",
    "\n",
    "    def generate_batch(self):\n",
    "        assert self.batch_size % self.num_skips == 0\n",
    "        assert self.num_skips <= 2 * self.skip_window\n",
    "        batch = np.ndarray(shape=(self.batch_size), dtype=np.int32)\n",
    "        labels = np.ndarray(shape=(self.batch_size, 1), dtype=np.int32)\n",
    "        # Compute the length/span of the scanning window [ self.skip_window target self.skip_window ]\n",
    "        span = 2 * self.skip_window + 1\n",
    "        buffer = collections.deque(maxlen=span)\n",
    "        # Reset the data_index to zero if the scanning window has reached the end of vocabulary.\n",
    "        if self.data_index + span > len(self.data):\n",
    "            self.data_index = 0\n",
    "        buffer.extend(self.data[self.data_index:self.data_index + span])\n",
    "        # Update the data_index value so that it points to end of scanning window.\n",
    "        self.data_index += span\n",
    "        for i in range(self.batch_size // self.num_skips):\n",
    "            # Get the indexes for outer context_words surronding the center word.\n",
    "#             context_words = [w for w in range(span) if w != self.skip_window]\n",
    "            context_words = []\n",
    "            for w in range(0, span):\n",
    "#             for w in range(math.floor(skip_window / 2), span):\n",
    "                if w == self.skip_window:\n",
    "                    continue\n",
    "                context_words.append(w)\n",
    "            # Select a sub sample of the 'numm_skips' words to be used from the context_words.\n",
    "            words_to_use = random.sample(context_words, self.num_skips)\n",
    "            for j, context_word in enumerate(words_to_use):\n",
    "                batch[i * self.num_skips + j] = buffer[self.skip_window]\n",
    "                labels[i * self.num_skips + j, 0] = buffer[context_word]\n",
    "            # Shift the scanning window to the right by inserting element pointed by data_index.\n",
    "            if self.data_index == len(self.data):\n",
    "                buffer.extend(self.data[0:span])\n",
    "                self.data_index = span\n",
    "            else:\n",
    "                buffer.append(self.data[self.data_index])\n",
    "                self.data_index += 1\n",
    "        # Adjust data_index by 'span' so that no words are skipped.\n",
    "        self.data_index = (self.data_index + len(self.data) - span) % len(self.data)\n",
    "        # batch(row vector) contains the 'self.batch_size' number of integer ids(context words)\n",
    "        # labels(column vector) contain the integer ids for the center/target words\n",
    "        return batch, labels\n",
    "\n",
    "    def build_model(self):\n",
    "        graph = tf.Graph()\n",
    "        with graph.as_default():\n",
    "            # Input data.\n",
    "            with tf.name_scope('inputs'):\n",
    "                self.train_inputs = tf.placeholder(tf.int32, shape=[self.batch_size])\n",
    "                self.train_labels = tf.placeholder(tf.int32, shape=[self.batch_size, 1])\n",
    "                valid_dataset = tf.constant(self.valid_examples, dtype=tf.int32)\n",
    "            # Ops and variables pinned to the CPU because of missing GPU implementation\n",
    "            with tf.device('/cpu:0'):\n",
    "                # Look up embeddings for inputs.\n",
    "                with tf.name_scope('embeddings'):\n",
    "                    embeddings = tf.Variable(tf.random_uniform([self.vocabulary_size, self.embedding_size], -1.0, 1.0))\n",
    "                    # We need to look up the embeddings corresponding to the integer ids in the training inputs.\n",
    "                    embed = tf.nn.embedding_lookup(embeddings, self.train_inputs)\n",
    "                # Construct the variables for the NCE loss\n",
    "                with tf.name_scope('weights'):\n",
    "                    nce_weights = tf.Variable(tf.truncated_normal([self.vocabulary_size, self.embedding_size], stddev=1.0 / math.sqrt(self.embedding_size)))\n",
    "                with tf.name_scope('biases'):\n",
    "                    nce_biases = tf.Variable(tf.zeros([self.vocabulary_size]))\n",
    "\n",
    "            # Define the loss as average NCE loss for the batch.\n",
    "            # tf.nce_loss automatically draws a new sample of the negative labels each time we evaluate the loss.\n",
    "            # Explanation of the meaning of NCE loss: http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/\n",
    "            with tf.name_scope('loss'):\n",
    "                self.loss = tf.reduce_mean(tf.nn.nce_loss(weights=nce_weights, biases=nce_biases,\n",
    "                                                     labels=self.train_labels, inputs=embed,\n",
    "                                                     num_sampled=self.num_sampled, num_classes=self.vocabulary_size))\n",
    "            # Construct the SGD optimizer using a learning rate of 1.0.\n",
    "            with tf.name_scope('optimizer'):\n",
    "                self.optimizer = tf.train.GradientDescentOptimizer(1.0).minimize(self.loss)\n",
    "            # Compute the cosine similarity between minibatch examples and all embeddings.\n",
    "            norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keepdims=True))\n",
    "            self.normalized_embeddings = embeddings / norm\n",
    "            valid_embeddings = tf.nn.embedding_lookup(self.normalized_embeddings, valid_dataset)\n",
    "            self.similarity = tf.matmul(valid_embeddings, self.normalized_embeddings, transpose_b=True)\n",
    "            # Add variable initializer.\n",
    "            init = tf.global_variables_initializer()\n",
    "        self.graph = graph\n",
    "        self.init = init\n",
    "        print('Success: Built Model.')\n",
    "\n",
    "    def train_model(self):\n",
    "        self.log_to_mlflow()\n",
    "        try:\n",
    "            with tf.Session(graph=self.graph) as session:\n",
    "                # We must initialize graph variables before we use them.\n",
    "                self.init.run()\n",
    "                print('Initialized Model Parameters.')\n",
    "                average_loss = 0\n",
    "                for step in xrange(self.num_steps):\n",
    "                    batch_inputs, batch_labels = self.generate_batch()\n",
    "                    feed_dict = {self.train_inputs: batch_inputs, self.train_labels: batch_labels}\n",
    "                    _, loss_val = session.run([self.optimizer, self.loss], feed_dict=feed_dict)\n",
    "                    mlflow.log_metric('step', step)\n",
    "                    mlflow.log_metric('loss', loss_val)\n",
    "                    average_loss += loss_val\n",
    "                    if step % 2000 == 0:\n",
    "                        if step > 0:\n",
    "                            average_loss /= 2000\n",
    "                        # The average loss is an estimate of the loss over the last 2000 batches.\n",
    "                        print('Average loss at step ', step, ': ', average_loss)\n",
    "                        average_loss = 0\n",
    "                    self.final_embeddings = self.normalized_embeddings.eval()\n",
    "                    if loss_val <= self.min_loss:\n",
    "                        self.min_loss = loss_val\n",
    "                        print('Min Loss :', self.min_loss)\n",
    "                        self.best_embeddings = self.final_embeddings\n",
    "            print('Success: Trained Model.')\n",
    "        finally:\n",
    "#             self.save_and_log_embeddings()\n",
    "            self.compute_distances()\n",
    "            self.compute_probabilities()\n",
    "\n",
    "    def visualize(self, filename='tsne.png'):\n",
    "        # Remove this in production as it is very expensive to compute.\n",
    "        def plot_with_labels(low_dim_embs, labels, filename):\n",
    "            assert low_dim_embs.shape[0] >= len(labels), 'More labels than embeddings'\n",
    "            plt.figure(figsize=(18, 18))  # in inches\n",
    "            for i, label in enumerate(labels):\n",
    "                x, y = low_dim_embs[i, :]\n",
    "                plt.scatter(x, y)\n",
    "                plt.annotate(label, xy=(x, y), xytext=(5, 2), textcoords='offset points', ha='right', va='bottom')\n",
    "            plt.savefig(filename)\n",
    "            plt.show()\n",
    "        try:\n",
    "            from sklearn.manifold import TSNE\n",
    "            import matplotlib.pyplot as plt\n",
    "            tsne = TSNE(perplexity=30, n_components=2, init='pca', n_iter=5000, method='exact')\n",
    "            plot_only = min(500, len(set(self.reverse_dictionary)))\n",
    "            low_dim_embs = tsne.fit_transform(self.final_embeddings[:plot_only, :])\n",
    "            labels = [self.reverse_dictionary[i] for i in xrange(plot_only)]\n",
    "            plot_with_labels(low_dim_embs, labels, os.path.join(os.getcwd(), filename))\n",
    "            mlflow.log_artifact(filename)\n",
    "        except ImportError as ex:\n",
    "            print('Please install sklearn, matplotlib, and scipy to show embeddings.')\n",
    "            print(ex)\n",
    "            \n",
    "    def save_and_log_embeddings(self):\n",
    "        classes_filename = 'output_classes.txt'\n",
    "        embeddings_filename = 'output_embeddings.txt'\n",
    "        classes = open(classes_filename, 'w')\n",
    "        embeddings = open(embeddings_filename, 'w')\n",
    "        for key in self.reverse_dictionary:\n",
    "            classes.write(str(self.reverse_dictionary[key]) + '\\n')\n",
    "            embeddings.write(' '.join(map(str, self.best_embeddings[0])) + '\\n')\n",
    "        classes.close()\n",
    "        embeddings.close()\n",
    "        mlflow.log_artifact(classes_filename)\n",
    "        mlflow.log_artifact(embeddings_filename)\n",
    "        print('Success: Saved Embeddings.')\n",
    "            \n",
    "    def log_to_mlflow(self):\n",
    "        mlflow.log_artifact(self.filename)\n",
    "        mlflow.log_param('vocabulary_size', self.vocabulary_size)\n",
    "        mlflow.log_param('batch_size', self.batch_size)\n",
    "        mlflow.log_param('skip_window', self.skip_window)\n",
    "        mlflow.log_param('num_skips', self.num_skips)\n",
    "        mlflow.log_param('embedding_size', self.embedding_size)\n",
    "        mlflow.log_param('num_sampled', self.num_sampled)\n",
    "        mlflow.log_param('num_steps', self.num_steps)\n",
    "    \n",
    "    def compute_distances(self):\n",
    "        self.distances = distance.cdist(self.best_embeddings, self.best_embeddings, 'euclidean')\n",
    "        print('Success: Computed Distances.')\n",
    "        \n",
    "    def compute_probabilities(self):\n",
    "        x = 1 / self.distances\n",
    "        np.fill_diagonal(x, 0)\n",
    "        self.probabilities = x / x.sum(axis=1, keepdims=True)\n",
    "        print('Success: Computed Probabilities.')\n",
    "        \n",
    "    def draw_distribution(self, num):\n",
    "        probs = []\n",
    "        vals = []\n",
    "        for key in self.unused_dictionary:\n",
    "            if key == 'UNK':\n",
    "                continue\n",
    "            vals.append(float(key))\n",
    "        vals = sorted(vals)\n",
    "        keys = []\n",
    "        for v in vals:\n",
    "            keys.append(self.unused_dictionary[str(v)])\n",
    "        for k in keys:\n",
    "            probs.append(self.probabilities[self.unused_dictionary[str(float(num))]][k])\n",
    "        vals_strings = []\n",
    "        for v in vals:\n",
    "            vals_strings.append(str(v))\n",
    "        p = figure(x_range=vals_strings, plot_height=500, plot_width=500, title=\"Return : \" + str(float(num)))\n",
    "        p.vbar(x=vals_strings, top=probs, width=0.9)\n",
    "        p.xgrid.grid_line_color = None\n",
    "        p.y_range.start = 0\n",
    "        mean = np.array(vals).mean()\n",
    "        std = np.array(vals).std()\n",
    "        print('Mean :', mean)\n",
    "        print('Std  :', abs(std))\n",
    "        p.vbar(x=[str(round(mean, 2))], top=[np.array(probs).max()], width=1.8, color='black')\n",
    "        p.vbar(x=[str(round(mean + abs(mean - std), 2))], top=[np.array(probs).max()], width=1.8, color='red')\n",
    "        p.vbar(x=[str(round(mean - abs(mean - std), 2))], top=[np.array(probs).max()], width=1.8, color='red')\n",
    "        show(p)\n",
    "        \n",
    "    def compute_expected_val_probs(self, num, required_prob = 0.1):\n",
    "        if str(float(num)) not in self.unused_dictionary:\n",
    "            return {}\n",
    "        vals = list(self.unused_dictionary.keys())[1:]\n",
    "        probs = self.probabilities[self.unused_dictionary[str(float(num))]][1:]\n",
    "        val_to_prob = {}\n",
    "        for i in range(len(vals)):\n",
    "            val_to_prob[vals[i]] = probs[i]\n",
    "        expected_val_probs = {}\n",
    "        p = 0\n",
    "        for k, v in sorted(val_to_prob.items(), key=lambda x: x[1], reverse=True):\n",
    "            if p >= required_prob:\n",
    "                break\n",
    "            expected_val_probs[float(k)] = v\n",
    "            p += v\n",
    "        return expected_val_probs\n",
    "\n",
    "    def breadth_search(self, n, change_percent=0.2, required_prob=0.1, top_samples=0):\n",
    "        l = [k for k in self.unused_dictionary][1:]\n",
    "        x = round((1 - change_percent) * n, 2)\n",
    "        y = []\n",
    "        while True:\n",
    "            if x <= round((1 + change_percent) * n, 2):\n",
    "                if str(x) in l:\n",
    "                    y.append(x)\n",
    "            else:\n",
    "                break\n",
    "            x = round(x + 0.01, 2)\n",
    "        z = []\n",
    "        for v in y:\n",
    "            if str(float(v)) in self.unused_dictionary:\n",
    "                z.append(v)\n",
    "        val_probs = {}\n",
    "        breadth_val_probs = {}\n",
    "        if top_samples == 0:\n",
    "            for num in z:\n",
    "                expected_val_probs = self.compute_expected_val_probs(num, required_prob)\n",
    "                for key in expected_val_probs:\n",
    "                    if key in val_probs:\n",
    "                        val_probs[key] += expected_val_probs[key]\n",
    "                    else:\n",
    "                        val_probs[key] = expected_val_probs[key]\n",
    "            for k, v in sorted(val_probs.items(), key=lambda x: x[1], reverse=True):\n",
    "                breadth_val_probs[k] = v / len(z)\n",
    "        else:\n",
    "            for num in z:\n",
    "                expected_val_probs = self.compute_expected_val_probs(num, 1)\n",
    "                for key in expected_val_probs:\n",
    "                    if key in val_probs:\n",
    "                        val_probs[key] += expected_val_probs[key]\n",
    "                    else:\n",
    "                        val_probs[key] = expected_val_probs[key]\n",
    "            total = 0\n",
    "            for key in val_probs:\n",
    "                total += val_probs[key]\n",
    "            normalized_val_probs = {}\n",
    "            for key in val_probs:\n",
    "                normalized_val_probs[key] = val_probs[key] / total\n",
    "            count = 0\n",
    "            for k, v in sorted(normalized_val_probs.items(), key=lambda x: x[1], reverse=True):\n",
    "                breadth_val_probs[k] = v\n",
    "                count += 1\n",
    "                if count >= top_samples:\n",
    "                    break\n",
    "        return breadth_val_probs\n",
    "\n",
    "    def depth_search(self, num_list, change_percent=0.2, required_prob=0.1):\n",
    "        current_val_probs = {}\n",
    "        for num in num_list:\n",
    "            breadth_val_probs = self.breadth_search(num, change_percent, 1, 0)\n",
    "            for key in breadth_val_probs:\n",
    "                if key in current_val_probs:\n",
    "                    current_val_probs[key] *= breadth_val_probs[key]\n",
    "                else:\n",
    "                    current_val_probs[key] = breadth_val_probs[key]\n",
    "            total = 0\n",
    "            for key in current_val_probs:\n",
    "                total += current_val_probs[key]\n",
    "            normalized_val_probs = {}\n",
    "            for key in current_val_probs:\n",
    "                normalized_val_probs[key] = current_val_probs[key] / total\n",
    "            current_val_probs = normalized_val_probs\n",
    "        depth_val_probs = {}\n",
    "        p = 0\n",
    "        for k, v in sorted(current_val_probs.items(), key=lambda x: x[1], reverse=True):\n",
    "            if p >= required_prob:\n",
    "                break\n",
    "            depth_val_probs[float(k)] = v\n",
    "            p += v\n",
    "        return depth_val_probs\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ticker = 'AAPL'\n",
    "filename = ticker + '_returns_train.txt'\n",
    "# Hyperparameters\n",
    "embedding_size = 300\n",
    "# # Use these in production.\n",
    "skip_window=20\n",
    "num_skips=16\n",
    "# vocabulary_size is the number of tokens with frquency greater than 1.\n",
    "batch_size=128\n",
    "num_sampled = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0702 16:28:46.797079 140081117902656 deprecation.py:323] From /home/demq/.anaconda3/envs/dq/lib/python3.7/site-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success: Loaded Vocabulary.\n",
      "Vocabulary size : 1549\n",
      "Success: Built Dataset.\n",
      "Success: Built Model.\n",
      "Initialized Model Parameters.\n",
      "Average loss at step  0 :  28.162139892578125\n",
      "Min Loss : 28.16214\n",
      "Min Loss : 17.595816\n",
      "Min Loss : 6.589919\n",
      "Min Loss : 3.537791\n",
      "Min Loss : 2.2679682\n",
      "Success: Computed Distances.\n",
      "Success: Computed Probabilities.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/demq/.anaconda3/envs/dq/lib/python3.7/site-packages/ipykernel_launcher.py:249: RuntimeWarning: divide by zero encountered in true_divide\n"
     ]
    },
    {
     "ename": "MlflowException",
     "evalue": "Got invalid value inf for metric 'loss' (timestamp=1562065130516). Please specify value as a valid double (64-bit floating point)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m----------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMlflowException\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-babe909b8900>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWord2Vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskip_window\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_skips\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedding_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_sampled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mmlflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexperiment_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_experiment\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-3-e8166222d3f2>\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mload_vocabulary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-e8166222d3f2>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    175\u001b[0m                     \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m                     \u001b[0mmlflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_metric\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'step'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 177\u001b[0;31m                     \u001b[0mmlflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_metric\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    178\u001b[0m                     \u001b[0maverage_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss_val\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m2000\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.anaconda3/envs/dq/lib/python3.7/site-packages/mlflow/tracking/fluent.py\u001b[0m in \u001b[0;36mlog_metric\u001b[0;34m(key, value, step)\u001b[0m\n\u001b[1;32m    192\u001b[0m     \"\"\"\n\u001b[1;32m    193\u001b[0m     \u001b[0mrun_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_or_start_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_id\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 194\u001b[0;31m     \u001b[0mMlflowClient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_metric\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.anaconda3/envs/dq/lib/python3.7/site-packages/mlflow/tracking/client.py\u001b[0m in \u001b[0;36mlog_metric\u001b[0;34m(self, run_id, key, value, timestamp, step)\u001b[0m\n\u001b[1;32m    160\u001b[0m         \u001b[0mtimestamp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtimestamp\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtimestamp\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m         \u001b[0mstep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 162\u001b[0;31m         \u001b[0m_validate_metric\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimestamp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    163\u001b[0m         \u001b[0mmetric\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMetric\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimestamp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_metric\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetric\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.anaconda3/envs/dq/lib/python3.7/site-packages/mlflow/utils/validation.py\u001b[0m in \u001b[0;36m_validate_metric\u001b[0;34m(key, value, timestamp, step)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;34m\"Got invalid value %s for metric '%s' (timestamp=%s). Please specify value as a valid \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;34m\"double (64-bit floating point)\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimestamp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             INVALID_PARAMETER_VALUE)\n\u001b[0m\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimestamp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumbers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNumber\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mtimestamp\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mMlflowException\u001b[0m: Got invalid value inf for metric 'loss' (timestamp=1562065130516). Please specify value as a valid double (64-bit floating point)"
     ]
    }
   ],
   "source": [
    "num_steps = 2001\n",
    "model = Word2Vec(filename, None, batch_size, skip_window, num_skips, embedding_size, num_sampled, num_steps)\n",
    "with mlflow.start_run(experiment_id=model_experiment):\n",
    "    model.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean : 0.0028007092531630857\n",
      "Std  : 0.018561790921332606\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "  <div class=\"bk-root\" id=\"0b57c4d1-791e-4111-ac29-5295d6612477\" data-root-id=\"1002\"></div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "(function(root) {\n",
       "  function embed_document(root) {\n",
       "    \n",
       "  var docs_json = {\"bec85b1d-f1e3-48c0-8a6b-dc4f020b25db\":{\"roots\":{\"references\":[{\"attributes\":{\"below\":[{\"id\":\"1013\",\"type\":\"CategoricalAxis\"}],\"center\":[{\"id\":\"1016\",\"type\":\"Grid\"},{\"id\":\"1021\",\"type\":\"Grid\"}],\"left\":[{\"id\":\"1017\",\"type\":\"LinearAxis\"}],\"plot_height\":500,\"plot_width\":500,\"renderers\":[{\"id\":\"1038\",\"type\":\"GlyphRenderer\"},{\"id\":\"1043\",\"type\":\"GlyphRenderer\"},{\"id\":\"1048\",\"type\":\"GlyphRenderer\"},{\"id\":\"1053\",\"type\":\"GlyphRenderer\"}],\"title\":{\"id\":\"1003\",\"type\":\"Title\"},\"toolbar\":{\"id\":\"1028\",\"type\":\"Toolbar\"},\"x_range\":{\"id\":\"1005\",\"type\":\"FactorRange\"},\"x_scale\":{\"id\":\"1009\",\"type\":\"CategoricalScale\"},\"y_range\":{\"id\":\"1007\",\"type\":\"DataRange1d\"},\"y_scale\":{\"id\":\"1011\",\"type\":\"LinearScale\"}},\"id\":\"1002\",\"subtype\":\"Figure\",\"type\":\"Plot\"},{\"attributes\":{\"formatter\":{\"id\":\"1058\",\"type\":\"CategoricalTickFormatter\"},\"ticker\":{\"id\":\"1014\",\"type\":\"CategoricalTicker\"}},\"id\":\"1013\",\"type\":\"CategoricalAxis\"},{\"attributes\":{},\"id\":\"1022\",\"type\":\"PanTool\"},{\"attributes\":{\"fill_color\":{\"value\":\"red\"},\"line_color\":{\"value\":\"red\"},\"top\":{\"field\":\"top\"},\"width\":{\"value\":1.8},\"x\":{\"field\":\"x\"}},\"id\":\"1051\",\"type\":\"VBar\"},{\"attributes\":{\"fill_alpha\":{\"value\":0.1},\"fill_color\":{\"value\":\"#1f77b4\"},\"line_alpha\":{\"value\":0.1},\"line_color\":{\"value\":\"#1f77b4\"},\"top\":{\"field\":\"top\"},\"width\":{\"value\":1.8},\"x\":{\"field\":\"x\"}},\"id\":\"1052\",\"type\":\"VBar\"},{\"attributes\":{\"data_source\":{\"id\":\"1050\",\"type\":\"ColumnDataSource\"},\"glyph\":{\"id\":\"1051\",\"type\":\"VBar\"},\"hover_glyph\":null,\"muted_glyph\":null,\"nonselection_glyph\":{\"id\":\"1052\",\"type\":\"VBar\"},\"selection_glyph\":null,\"view\":{\"id\":\"1054\",\"type\":\"CDSView\"}},\"id\":\"1053\",\"type\":\"GlyphRenderer\"},{\"attributes\":{\"dimension\":1,\"ticker\":{\"id\":\"1018\",\"type\":\"BasicTicker\"}},\"id\":\"1021\",\"type\":\"Grid\"},{\"attributes\":{},\"id\":\"1027\",\"type\":\"HelpTool\"},{\"attributes\":{\"source\":{\"id\":\"1050\",\"type\":\"ColumnDataSource\"}},\"id\":\"1054\",\"type\":\"CDSView\"},{\"attributes\":{\"fill_alpha\":{\"value\":0.1},\"fill_color\":{\"value\":\"#1f77b4\"},\"line_alpha\":{\"value\":0.1},\"line_color\":{\"value\":\"#1f77b4\"},\"top\":{\"field\":\"top\"},\"width\":{\"value\":1.8},\"x\":{\"field\":\"x\"}},\"id\":\"1042\",\"type\":\"VBar\"},{\"attributes\":{\"source\":{\"id\":\"1040\",\"type\":\"ColumnDataSource\"}},\"id\":\"1044\",\"type\":\"CDSView\"},{\"attributes\":{\"callback\":null,\"start\":0},\"id\":\"1007\",\"type\":\"DataRange1d\"},{\"attributes\":{\"fill_color\":{\"value\":\"black\"},\"top\":{\"field\":\"top\"},\"width\":{\"value\":1.8},\"x\":{\"field\":\"x\"}},\"id\":\"1041\",\"type\":\"VBar\"},{\"attributes\":{},\"id\":\"1023\",\"type\":\"WheelZoomTool\"},{\"attributes\":{},\"id\":\"1056\",\"type\":\"BasicTickFormatter\"},{\"attributes\":{\"fill_color\":{\"value\":\"#1f77b4\"},\"line_color\":{\"value\":\"#1f77b4\"},\"top\":{\"field\":\"top\"},\"width\":{\"value\":0.9},\"x\":{\"field\":\"x\"}},\"id\":\"1036\",\"type\":\"VBar\"},{\"attributes\":{\"source\":{\"id\":\"1035\",\"type\":\"ColumnDataSource\"}},\"id\":\"1039\",\"type\":\"CDSView\"},{\"attributes\":{\"source\":{\"id\":\"1045\",\"type\":\"ColumnDataSource\"}},\"id\":\"1049\",\"type\":\"CDSView\"},{\"attributes\":{},\"id\":\"1009\",\"type\":\"CategoricalScale\"},{\"attributes\":{\"callback\":null,\"data\":{\"top\":[0.04815981563041891],\"x\":[\"0.0\"]},\"selected\":{\"id\":\"1063\",\"type\":\"Selection\"},\"selection_policy\":{\"id\":\"1062\",\"type\":\"UnionRenderers\"}},\"id\":\"1040\",\"type\":\"ColumnDataSource\"},{\"attributes\":{\"data_source\":{\"id\":\"1045\",\"type\":\"ColumnDataSource\"},\"glyph\":{\"id\":\"1046\",\"type\":\"VBar\"},\"hover_glyph\":null,\"muted_glyph\":null,\"nonselection_glyph\":{\"id\":\"1047\",\"type\":\"VBar\"},\"selection_glyph\":null,\"view\":{\"id\":\"1049\",\"type\":\"CDSView\"}},\"id\":\"1048\",\"type\":\"GlyphRenderer\"},{\"attributes\":{\"callback\":null,\"data\":{\"top\":[0.02351496021771081,0.02333074314525798,0.023186207574890393,0.023724238164273447,0.023912525078517273,0.02333789087816633,0.02386238533057302,0.02447464183656387,0.02287623792626573,0.023989724362399042,0.024125246167320924,0.023920381853242655,0.024209155219996857,0.023438006558336118,0.024217813475642187,0.024879120307368285,0.024057097502931696,0.0,0.02393709353420073,0.024270435792541244,0.023572914635061715,0.02516447886763756,0.0231660933523658,0.02346225747184412,0.02386932534271375,0.02282404997699881,0.024548866486847405,0.024112275247493695,0.023604556907992152,0.024152103182318575,0.02382678053535791,0.023269392908341142,0.023962596666470482,0.0295024354557737,0.02278977719324015,0.024571651436463973,0.04815981563041891,0.02316912341097122,0.02374630551865682,0.023591022275143498],\"x\":[\"-0.039968025579528416\",\"-0.030286204633790455\",\"-0.02491652962574523\",\"-0.01999800019999023\",\"-0.015329586101175848\",\"-0.015057973196808282\",\"-0.015020277374456084\",\"-0.015001500150015572\",\"-0.010046212577863286\",\"-0.010017028949218788\",\"-0.010016527269986383\",\"-0.010003501225434017\",\"-0.009999000099995116\",\"-0.009993004896563311\",\"-0.005024368185696079\",\"-0.005000250012510288\",\"-0.0025008753063549506\",\"0.0\",\"0.0025008127641460735\",\"0.004993259100210168\",\"0.004998000799675582\",\"0.005001500450130492\",\"0.005001750612709901\",\"0.005002000800329795\",\"0.005022349455070516\",\"0.010002000400070918\",\"0.010003000900275198\",\"0.010013016921989491\",\"0.010044194455595526\",\"0.010051261433315023\",\"0.015067805123054312\",\"0.01999300244915303\",\"0.020015011258439852\",\"0.02002202422664533\",\"0.030006001200241186\",\"0.0300691590658526\",\"0.03256349882270313\",\"0.03489879349884992\",\"0.035038542396632885\",\"0.03990024937656484\"]},\"selected\":{\"id\":\"1061\",\"type\":\"Selection\"},\"selection_policy\":{\"id\":\"1060\",\"type\":\"UnionRenderers\"}},\"id\":\"1035\",\"type\":\"ColumnDataSource\"},{\"attributes\":{},\"id\":\"1060\",\"type\":\"UnionRenderers\"},{\"attributes\":{\"active_drag\":\"auto\",\"active_inspect\":\"auto\",\"active_multi\":null,\"active_scroll\":\"auto\",\"active_tap\":\"auto\",\"tools\":[{\"id\":\"1022\",\"type\":\"PanTool\"},{\"id\":\"1023\",\"type\":\"WheelZoomTool\"},{\"id\":\"1024\",\"type\":\"BoxZoomTool\"},{\"id\":\"1025\",\"type\":\"SaveTool\"},{\"id\":\"1026\",\"type\":\"ResetTool\"},{\"id\":\"1027\",\"type\":\"HelpTool\"}]},\"id\":\"1028\",\"type\":\"Toolbar\"},{\"attributes\":{},\"id\":\"1062\",\"type\":\"UnionRenderers\"},{\"attributes\":{},\"id\":\"1025\",\"type\":\"SaveTool\"},{\"attributes\":{},\"id\":\"1018\",\"type\":\"BasicTicker\"},{\"attributes\":{},\"id\":\"1063\",\"type\":\"Selection\"},{\"attributes\":{},\"id\":\"1011\",\"type\":\"LinearScale\"},{\"attributes\":{\"fill_alpha\":{\"value\":0.1},\"fill_color\":{\"value\":\"#1f77b4\"},\"line_alpha\":{\"value\":0.1},\"line_color\":{\"value\":\"#1f77b4\"},\"top\":{\"field\":\"top\"},\"width\":{\"value\":0.9},\"x\":{\"field\":\"x\"}},\"id\":\"1037\",\"type\":\"VBar\"},{\"attributes\":{},\"id\":\"1064\",\"type\":\"UnionRenderers\"},{\"attributes\":{\"callback\":null,\"data\":{\"top\":[0.04815981563041891],\"x\":[\"-0.01\"]},\"selected\":{\"id\":\"1067\",\"type\":\"Selection\"},\"selection_policy\":{\"id\":\"1066\",\"type\":\"UnionRenderers\"}},\"id\":\"1050\",\"type\":\"ColumnDataSource\"},{\"attributes\":{\"data_source\":{\"id\":\"1035\",\"type\":\"ColumnDataSource\"},\"glyph\":{\"id\":\"1036\",\"type\":\"VBar\"},\"hover_glyph\":null,\"muted_glyph\":null,\"nonselection_glyph\":{\"id\":\"1037\",\"type\":\"VBar\"},\"selection_glyph\":null,\"view\":{\"id\":\"1039\",\"type\":\"CDSView\"}},\"id\":\"1038\",\"type\":\"GlyphRenderer\"},{\"attributes\":{},\"id\":\"1065\",\"type\":\"Selection\"},{\"attributes\":{\"formatter\":{\"id\":\"1056\",\"type\":\"BasicTickFormatter\"},\"ticker\":{\"id\":\"1018\",\"type\":\"BasicTicker\"}},\"id\":\"1017\",\"type\":\"LinearAxis\"},{\"attributes\":{\"fill_color\":{\"value\":\"red\"},\"line_color\":{\"value\":\"red\"},\"top\":{\"field\":\"top\"},\"width\":{\"value\":1.8},\"x\":{\"field\":\"x\"}},\"id\":\"1046\",\"type\":\"VBar\"},{\"attributes\":{},\"id\":\"1066\",\"type\":\"UnionRenderers\"},{\"attributes\":{},\"id\":\"1058\",\"type\":\"CategoricalTickFormatter\"},{\"attributes\":{\"text\":\"Return : 0.0\"},\"id\":\"1003\",\"type\":\"Title\"},{\"attributes\":{},\"id\":\"1067\",\"type\":\"Selection\"},{\"attributes\":{\"data_source\":{\"id\":\"1040\",\"type\":\"ColumnDataSource\"},\"glyph\":{\"id\":\"1041\",\"type\":\"VBar\"},\"hover_glyph\":null,\"muted_glyph\":null,\"nonselection_glyph\":{\"id\":\"1042\",\"type\":\"VBar\"},\"selection_glyph\":null,\"view\":{\"id\":\"1044\",\"type\":\"CDSView\"}},\"id\":\"1043\",\"type\":\"GlyphRenderer\"},{\"attributes\":{\"fill_alpha\":{\"value\":0.1},\"fill_color\":{\"value\":\"#1f77b4\"},\"line_alpha\":{\"value\":0.1},\"line_color\":{\"value\":\"#1f77b4\"},\"top\":{\"field\":\"top\"},\"width\":{\"value\":1.8},\"x\":{\"field\":\"x\"}},\"id\":\"1047\",\"type\":\"VBar\"},{\"attributes\":{\"callback\":null,\"factors\":[\"-0.039968025579528416\",\"-0.030286204633790455\",\"-0.02491652962574523\",\"-0.01999800019999023\",\"-0.015329586101175848\",\"-0.015057973196808282\",\"-0.015020277374456084\",\"-0.015001500150015572\",\"-0.010046212577863286\",\"-0.010017028949218788\",\"-0.010016527269986383\",\"-0.010003501225434017\",\"-0.009999000099995116\",\"-0.009993004896563311\",\"-0.005024368185696079\",\"-0.005000250012510288\",\"-0.0025008753063549506\",\"0.0\",\"0.0025008127641460735\",\"0.004993259100210168\",\"0.004998000799675582\",\"0.005001500450130492\",\"0.005001750612709901\",\"0.005002000800329795\",\"0.005022349455070516\",\"0.010002000400070918\",\"0.010003000900275198\",\"0.010013016921989491\",\"0.010044194455595526\",\"0.010051261433315023\",\"0.015067805123054312\",\"0.01999300244915303\",\"0.020015011258439852\",\"0.02002202422664533\",\"0.030006001200241186\",\"0.0300691590658526\",\"0.03256349882270313\",\"0.03489879349884992\",\"0.035038542396632885\",\"0.03990024937656484\"]},\"id\":\"1005\",\"type\":\"FactorRange\"},{\"attributes\":{\"overlay\":{\"id\":\"1068\",\"type\":\"BoxAnnotation\"}},\"id\":\"1024\",\"type\":\"BoxZoomTool\"},{\"attributes\":{\"bottom_units\":\"screen\",\"fill_alpha\":{\"value\":0.5},\"fill_color\":{\"value\":\"lightgrey\"},\"left_units\":\"screen\",\"level\":\"overlay\",\"line_alpha\":{\"value\":1.0},\"line_color\":{\"value\":\"black\"},\"line_dash\":[4,4],\"line_width\":{\"value\":2},\"render_mode\":\"css\",\"right_units\":\"screen\",\"top_units\":\"screen\"},\"id\":\"1068\",\"type\":\"BoxAnnotation\"},{\"attributes\":{\"callback\":null,\"data\":{\"top\":[0.04815981563041891],\"x\":[\"0.02\"]},\"selected\":{\"id\":\"1065\",\"type\":\"Selection\"},\"selection_policy\":{\"id\":\"1064\",\"type\":\"UnionRenderers\"}},\"id\":\"1045\",\"type\":\"ColumnDataSource\"},{\"attributes\":{},\"id\":\"1026\",\"type\":\"ResetTool\"},{\"attributes\":{},\"id\":\"1061\",\"type\":\"Selection\"},{\"attributes\":{},\"id\":\"1014\",\"type\":\"CategoricalTicker\"},{\"attributes\":{\"grid_line_color\":null,\"ticker\":{\"id\":\"1014\",\"type\":\"CategoricalTicker\"}},\"id\":\"1016\",\"type\":\"Grid\"}],\"root_ids\":[\"1002\"]},\"title\":\"Bokeh Application\",\"version\":\"1.2.0\"}};\n",
       "  var render_items = [{\"docid\":\"bec85b1d-f1e3-48c0-8a6b-dc4f020b25db\",\"roots\":{\"1002\":\"0b57c4d1-791e-4111-ac29-5295d6612477\"}}];\n",
       "  root.Bokeh.embed.embed_items_notebook(docs_json, render_items);\n",
       "\n",
       "  }\n",
       "  if (root.Bokeh !== undefined) {\n",
       "    embed_document(root);\n",
       "  } else {\n",
       "    var attempts = 0;\n",
       "    var timer = setInterval(function(root) {\n",
       "      if (root.Bokeh !== undefined) {\n",
       "        embed_document(root);\n",
       "        clearInterval(timer);\n",
       "      }\n",
       "      attempts++;\n",
       "      if (attempts > 100) {\n",
       "        console.log(\"Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing\");\n",
       "        clearInterval(timer);\n",
       "      }\n",
       "    }, 10, root)\n",
       "  }\n",
       "})(window);"
      ],
      "application/vnd.bokehjs_exec.v0+json": ""
     },
     "metadata": {
      "application/vnd.bokehjs_exec.v0+json": {
       "id": "1002"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.draw_distribution(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('vocabulary_snp_returns.txt', 'r')\n",
    "new_returns = []\n",
    "for l in f.readlines():\n",
    "    for r in l.split():\n",
    "        if r.strip() != \"\":\n",
    "            new_returns.append(float(r))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "future_window = skip_window\n",
    "change_percent = 0.2\n",
    "required_prob = 1\n",
    "leader_persistence = 2\n",
    "confusion_matrix = {'TP' : 0, 'FP' : 0, 'TN' : 0, 'FN' : 0}\n",
    "tp_fp_leader = None\n",
    "tn_fn_leader = None\n",
    "tp_fp_leader_counter = 0\n",
    "tn_fn_leader_counter = 0\n",
    "i = -1\n",
    "win, loss, alpha = 0, 0, 0\n",
    "while True:\n",
    "    i += 1\n",
    "    if (win + loss) != 0:\n",
    "        alpha = round(win / (win + loss) * 100, 2)\n",
    "        print(alpha)\n",
    "    if i >= (len(new_returns) - 1):\n",
    "        break\n",
    "    if i < future_window:\n",
    "        continue\n",
    "    present = new_returns[(i - future_window):i]\n",
    "    past = new_returns[(i - 2 * future_window):(i - future_window)]\n",
    "    future = new_returns[i:(i + future_window)]\n",
    "    if past == []:\n",
    "        continue\n",
    "#     future_expectations = list(model.depth_search(present, change_percent, required_prob).keys())    \n",
    "#     p, n = 0, 0\n",
    "#     for v in future_expectations:\n",
    "#         if v > 0:\n",
    "#             p += abs(v)\n",
    "#         else:\n",
    "#             n += abs(v)\n",
    "    past_mean = np.array(past).mean()\n",
    "    future_mean = np.array(future).mean()\n",
    "    if past_mean > 0 and future_mean < 0:\n",
    "        win += 1\n",
    "    elif past_mean < 0 and future_mean > 0:\n",
    "        win += 1\n",
    "    else:\n",
    "        loss += 1\n",
    "    continue\n",
    "    if p > n:\n",
    "        if tp_fp_leader != None:\n",
    "            # If Leader is False, change the sign of the mean.\n",
    "            if tp_fp_leader == 'FP':\n",
    "                past_mean = -past_mean\n",
    "            if past_mean > 0:\n",
    "                confusion_matrix['TP'] += 1\n",
    "                if tp_fp_leader == 'TP':\n",
    "                    win += 1\n",
    "                else:\n",
    "                    loss += 1\n",
    "                if tp_fp_leader != 'TP':\n",
    "                    tp_fp_leader_counter += 1\n",
    "                    if tp_fp_leader_counter % leader_persistence == 0:\n",
    "                        tp_fp_leader = 'TP'\n",
    "            elif past_mean < 0:\n",
    "                confusion_matrix['FP'] += 1\n",
    "                if tp_fp_leader == 'FP':\n",
    "                    win += 1\n",
    "                else:\n",
    "                    loss += 1\n",
    "                if tp_fp_leader != 'FP':\n",
    "                    tp_fp_leader_counter += 1\n",
    "                    if tp_fp_leader_counter % leader_persistence == 0:\n",
    "                        tp_fp_leader = 'FP'\n",
    "        else:\n",
    "            # TP_FP_LEADER has not been decided.\n",
    "            tp_fp_leader_counter += 1\n",
    "            if past_mean > 0:\n",
    "                confusion_matrix['TP'] += 1\n",
    "                tp_fp_leader = 'TP'\n",
    "                win += 1\n",
    "            elif past_mean < 0:\n",
    "                confusion_matrix['FP'] += 1\n",
    "                tp_fp_leader = 'FP'\n",
    "                loss += 1\n",
    "    elif p < n:\n",
    "        if tn_fn_leader != None:\n",
    "            # If Leader is False, change the sign of the mean.\n",
    "            if tn_fn_leader == 'FN':\n",
    "                past_mean = -past_mean\n",
    "            if past_mean < 0:\n",
    "                confusion_matrix['TN'] += 1\n",
    "                if tn_fn_leader == 'TN':\n",
    "                    win += 1\n",
    "                else:\n",
    "                    loss += 1\n",
    "                if tn_fn_leader != 'TN':\n",
    "                    tn_fn_leader_counter += 1\n",
    "                    if tn_fn_leader_counter % leader_persistence == 0:\n",
    "                        tn_fn_leader = 'TN'\n",
    "            elif past_mean > 0:\n",
    "                confusion_matrix['FN'] += 1\n",
    "                if tn_fn_leader == 'FN':\n",
    "                    win += 1\n",
    "                else:\n",
    "                    loss += 1\n",
    "                if tn_fn_leader != 'FN':\n",
    "                    tn_fn_leader_counter += 1\n",
    "                    if tn_fn_leader_counter % leader_persistence == 0:\n",
    "                        tn_fn_leader = 'FN'\n",
    "        else:\n",
    "            # TN_FN_LEADER has not been decided.\n",
    "            tn_fn_leader_counter += 1\n",
    "            if past_mean < 0:\n",
    "                confusion_matrix['TN'] += 1\n",
    "                tn_fn_leader = 'TN'\n",
    "                win += 1\n",
    "            elif past_mean > 0:\n",
    "                confusion_matrix['FN'] += 1\n",
    "                tn_fn_leader = 'FN'\n",
    "                loss += 1\n",
    "    else:\n",
    "        continue\n",
    "print(confusion_matrix, win, loss, alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
